{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sheets=pd.read_excel('content.xlsx',None)\n",
    "#df_all=pd.concat(sheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download() #download nltk once\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.summarization.summarizer import summarize as gsum\n",
    "from gensim.summarization import keywords\n",
    "from summa.summarizer import summarize as ssum\n",
    "\n",
    "import spacy\n",
    "data={'doc_id':[],'sheet_id':[],'text':[]}\n",
    "for s in sheets.keys():\n",
    "    for k,v in sheets[s].iterrows():\n",
    "        try:\n",
    "            if len(v['text'])>10:\n",
    "                d=v['doc_id']\n",
    "                a=v['text']\n",
    "                data['doc_id'].append(d)   \n",
    "                data['sheet_id'].append(s)\n",
    "                data['text'].append(a)\n",
    "\n",
    "    #             sentences=[]\n",
    "    #             for s in sent_tokenize(a):\n",
    "    #                 names=[]\n",
    "    #                 for e in nlp(s).ents if e.label_ not in exclude:\n",
    "    #                     names.appendend((e.text,e.label_))\n",
    "    #                 sentences.append(s,names)\n",
    "            else:\n",
    "                print(d,'abstract too short',a)\n",
    "        except:\n",
    "            print(v['doc_id'],'unable to read abstract')\n",
    "df=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "encoding=\"utf-8\"\n",
    "corpus_file=\"corpus.txt\"\n",
    "dict_file=\"frequency_dictionary.txt\"\n",
    "dict_min_freq=2\n",
    "\n",
    "bigram_file=\"frequency_bigram.txt\"\n",
    "bigram_min_freq=2\n",
    "\n",
    "max_edit_distance_dictionary = 2\n",
    "max_edit_distance_lookup = 2\n",
    "prefix_length = 7\n",
    "\n",
    "output_json='corpus.json'\n",
    "\n",
    "with open(corpus_file, \"w+\", encoding=encoding) as f:\n",
    "    for k,(p,sh,a) in df.iterrows():\n",
    "        #print(\"{}\".format(a))\n",
    "        #print(a)\n",
    "        f.write(\"{}\\n\\n\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict\n",
    "#http://www.javascriptthai.com/wp-content/uploads/2013/11/Windows-874-Unicode-1.gif\n",
    "import os\n",
    "import re\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "dict_fd={}\n",
    "bigram_fd={}\n",
    "def parse_words(text):\n",
    "    matches = re.findall(r\"(([^\\W_]|['â€™-])+)\", text.lower())\n",
    "    #matches+= re.findall(r\"(?::?%[nmp])\", text)\n",
    "    #matches+= re.findall(r\"(^[u0E01-u0E5B]+$/)\", text)\n",
    "    #(?::?%[nmp])\n",
    "    #[u0E01-u0E5B]\n",
    "    #[^\\u0E00-\\u0E7F]\n",
    "    #[^\\u0E00-\\u0E7Fa-zA-Z' ]\n",
    "    matches = [match[0] for match in matches]\n",
    "    #print(matches)\n",
    "    return matches\n",
    "\n",
    "def sort_dict_by_value(x, reverse=True):\n",
    "    return {k: v for k, v in sorted(x.items(), key=lambda item: item[1], reverse=reverse)}\n",
    "\n",
    "def create_dict_from_sent(text,min_freq=1):\n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens=parse_words(text)\n",
    "    for w in tokens:\n",
    "        dict_fd[w]=dict_fd.get(w,0)+1\n",
    "        \n",
    "def create_dict(corpus_file, dict_file=\"frequency_dict.txt\", encoding=encoding, min_freq=1):\n",
    "    if not os.path.exists(corpus_file):\n",
    "        return False\n",
    "    with open(corpus_file, \"r\", encoding=encoding) as infile:\n",
    "        for para in infile:\n",
    "            sents=sent_tokenize(para)\n",
    "            for text in sents:\n",
    "                create_dict_from_sent(text)\n",
    "    with open(dict_file, \"w+\", encoding=encoding) as f:\n",
    "        for word,count in sort_dict_by_value( dict_fd, reverse=True ).items():\n",
    "            if count>=min_freq:\n",
    "                #print(\"{} {}\".format(word,count))\n",
    "                f.write(\"{} {}\\n\".format(word, count))\n",
    "                \n",
    "def create_bigram_from_sent(text,window_size=2,min_freq=1):\n",
    "    #tokens = word_tokenize(text)\n",
    "    tokens=parse_words(text)\n",
    "    finder = BigramCollocationFinder.from_words(tokens,window_size=window_size)\n",
    "    finder.apply_freq_filter(min_freq=min_freq)\n",
    "    for pair,count in sort_dict_by_value( finder.ngram_fd, reverse=True).items():\n",
    "        #print(pair,count)\n",
    "        bigram_fd[pair]=bigram_fd.get(pair,0)+1\n",
    "        \n",
    "def create_bigram(corpus_file, bigram_file=\"frequency_bigram.txt\", encoding=encoding,min_freq=1,window_size=2):\n",
    "    if not os.path.exists(corpus_file):\n",
    "        return False\n",
    "    with open(corpus_file, \"r\", encoding=encoding) as infile:\n",
    "        for para in infile:\n",
    "            sents=sent_tokenize(para)\n",
    "            for text in sents:\n",
    "                create_bigram_from_sent(text,window_size=window_size)\n",
    "    with open(bigram_file, \"w+\", encoding=encoding) as f:\n",
    "        for pair, count in sort_dict_by_value( dict(bigram_fd), reverse=True ).items():\n",
    "            if count>=min_freq:\n",
    "                #print(\"{} {}\".format(pair, count))\n",
    "                f.write(\"{} {} {}\\n\".format(pair[0],pair[1], count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dict(corpus_file=corpus_file, dict_file=dict_file, min_freq=dict_min_freq);\n",
    "create_bigram(corpus_file=corpus_file, bigram_file=bigram_file, min_freq=bigram_min_freq);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "exclude=['PERCENT','ORDINAL','CARDINAL']\n",
    "\n",
    "def get_keyword(text):\n",
    "    sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "    if not sym_spell.load_dictionary(dict_file, term_index=0, count_index=1):\n",
    "        print(\"Dictionary file not found\")\n",
    "    if not sym_spell.load_bigram_dictionary(bigram_file, term_index=0, count_index=2):\n",
    "        print(\"Bigram dictionary file not found\")    \n",
    "    suggestions=sym_spell.lookup_compound(text, max_edit_distance_lookup)\n",
    "    kw=keywords(suggestions[0].term, words=20, split=True, scores=False, lemmatize=True)\n",
    "    return kw\n",
    "\n",
    "#     'sentence': [\n",
    "#         (s, [ (e.text,e.label_) for e in nlp(s).ents if e.label_ not in exclude ])\n",
    "#         for s in sent_tokenize(a)\\\n",
    "#     ],\n",
    "def extract(df):\n",
    "    return [{\n",
    "            'doc_id': p,\n",
    "            'eng_abstract': a,\n",
    "            'keywords': get_keyword(a),\n",
    "            'name': sort_dict_by_value(Counter([e.text for e in nlp(a).ents if e.label_ not in exclude])),\n",
    "            'summarization':ssum(a),\n",
    "        } for k,(p,sh,a) in df.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "%%timeit -n1\n",
    "df2=df[:8]\n",
    "import multiprocessing\n",
    "n=multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool()\n",
    "list(pool.map( extract,[df2[df2.index%i==0] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sheet_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/sport/287.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>France coach Bernard Laporte has made four cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/sport/253.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>Celtic's Henri Camara and Nacho Novo of Ranger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/sport/012.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>World outdoor triple jump record holder and BB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/sport/232.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>Glenn Hoddle has been unveiled as the new Wolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/sport/202.txt</td>\n",
       "      <td>sport</td>\n",
       "      <td>Tottenham manager Jacques Santini has resigned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2219</th>\n",
       "      <td>data/tech/091.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>The body that represents the US movie industry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>data/tech/200.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Your computer can now help solve the world's m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>data/tech/173.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>The US is poised to push Japan off the top of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>data/tech/393.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Internet search engine users are an odd mix of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>data/tech/279.txt</td>\n",
       "      <td>tech</td>\n",
       "      <td>Music and film fans will be able to control th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2224 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  doc_id sheet_id  \\\n",
       "0     data/sport/287.txt    sport   \n",
       "1     data/sport/253.txt    sport   \n",
       "2     data/sport/012.txt    sport   \n",
       "3     data/sport/232.txt    sport   \n",
       "4     data/sport/202.txt    sport   \n",
       "...                  ...      ...   \n",
       "2219   data/tech/091.txt     tech   \n",
       "2220   data/tech/200.txt     tech   \n",
       "2221   data/tech/173.txt     tech   \n",
       "2222   data/tech/393.txt     tech   \n",
       "2223   data/tech/279.txt     tech   \n",
       "\n",
       "                                                   text  \n",
       "0     France coach Bernard Laporte has made four cha...  \n",
       "1     Celtic's Henri Camara and Nacho Novo of Ranger...  \n",
       "2     World outdoor triple jump record holder and BB...  \n",
       "3     Glenn Hoddle has been unveiled as the new Wolv...  \n",
       "4     Tottenham manager Jacques Santini has resigned...  \n",
       "...                                                 ...  \n",
       "2219  The body that represents the US movie industry...  \n",
       "2220  Your computer can now help solve the world's m...  \n",
       "2221  The US is poised to push Japan off the top of ...  \n",
       "2222  Internet search engine users are an odd mix of...  \n",
       "2223  Music and film fans will be able to control th...  \n",
       "\n",
       "[2224 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df\n",
    "from functools import reduce\n",
    "import multiprocessing\n",
    "n=multiprocessing.cpu_count()\n",
    "pool = multiprocessing.Pool()\n",
    "content=reduce(lambda x,y:x+y, pool.map( extract,[df2[df2.index%i==0] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(output_json, 'w+',encoding=encoding) as fp:\n",
    "    json.dump(content, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_json, 'r', encoding=encoding) as fp:\n",
    "    content2 = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
